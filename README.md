# llm-inference

Simple python code that can run inference on LLM models with rest api interface

## .env values and parameters

Default parameters:

```
SERVER_HOST = 127.0.0.1
SERVER_PORT = 5050
LOG_LEVEL = info

MODEL_PATH = /path/to/model 
ENABLE_CUDA = true
ENABLE_USE_CACHE = true
LOW_CPU_MEM_USAGE = true

MODEL_SEED = 42
MODEL_DEFAULT_NUM_BEAMS = 1
MODEL_DEFAULT_DO_SAMPLE = false
MODEL_DEFAULT_TEMPERATURE = 1.0
MODEL_DEFAULT_TOP_P = 1.0
MODEL_DEFAULT_TOP_K = 50

MODEL_DEFAULT_MAX_NEW_TOKENS = 4096
MODEL_DEFAULT_REPETITION_PENALTY = 1.0
MODEL_DEFAULT_LENGTH_PENALTY = 1.0
```

Rest API server config:

- **SERVER_HOST**:String - IP address the port will listen to (0.0.0.0 is any ip)
- **SERVER_PORT**:Int - Port the rest api service will listen to.
- **LOG_LEVEL**:String - Level of logs: critical, fatal, error, warning, warn, info, debug

General config:

- **MODEL_PATH**:String - Path to the model itself relative to the project.
- **ENABLE_CUDA**:Bool - Will try to run the model on the GPU if supported but will default to CPU if cuda is not
  supported.
- **ENABLE_USE_CACHE**:Bool - Weather or not to use caching for already calculated tokens, useful when pushing one token
  at a time
- **LOW_CPU_MEM_USAGE**:Bool - Exchange slower loading for less memory usage when loading the model to GPU


Default generation config:
- **MODEL_SEED**:Int - A seed in computing is a numerical value used to initialize a pseudorandom number generator,
  ensuring reproducibility of random sequences generated by the algorithm.
- **MODEL_DEFAULT_NUM_BEAMS**:Int- Number of different paths the model considers in parallel during beam search,
  influencing the diversity and quality of the generated text.
- **MODEL_DEFAULT_DO_SAMPLE**:Bool - Controls whether the model generates text by sampling from the probability
  distribution of each next word (when True), as opposed to just picking the most probable next word (when False).
- **MODEL_DEFAULT_TEMPERATURE**:Float - Hyperparameter that controls the randomness of predictions
- **MODEL_DEFAULT_TOP_P**:Float - Parameter used for nucleus sampling, controlling the randomness of output by only
  considering the smallest set of words whose cumulative probability exceeds the value p, thereby filtering out less
  likely words.
- **MODEL_DEFAULT_TOP_K**:Int - Parameter that limits the selection to the top k most probable next words, balancing the
  randomness and predictability of the generated text.

Default generation limits and penalties:

- **MODEL_DEFAULT_MAX_NEW_TOKENS**:Int - Max **new** tokens to generate per prompt request
- **MODEL_DEFAULT_REPETITION_PENALTY**:Float - Parameter used to discourage the model from repeating the same words or
  phrases, increasing the diversity of the generated text.
- **MODEL_DEFAULT_LENGTH_PENALTY**:Float - Parameter that adjusts the model's preference for longer or shorter
  sequences, with values greater than 1 favoring longer sequences and values less than 1 favoring shorter ones.

## Virtual environment activation

windows:

```
.\venv\Scripts\activate
```

Linux

```
source venv/bin/activate

```